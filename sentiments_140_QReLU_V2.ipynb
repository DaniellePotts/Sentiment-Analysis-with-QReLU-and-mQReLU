{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiments 140 QReLU V2.ipynb",
      "provenance": [],
      "mount_file_id": "1hu1eAZSAuzIbmfsIFpXJAb9Q_BHPOu9d",
      "authorship_tag": "ABX9TyPizZ781hRDfbYNvoTkSenN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaniellePotts/Sentiment-Analysis-with-QReLU-and-mQReLU/blob/main/sentiments_140_QReLU_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80GkflhOkMQr",
        "outputId": "9b44e85f-3962-4984-fe95-982ffa3ab627"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRtMCLtH9Ix9",
        "outputId": "9a3e3e56-71d1-4db2-ab9f-70f0d8f607aa"
      },
      "source": [
        "!pip install contractions\n",
        "!python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 61.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85445 sha256=118065938fc7af86a0027cdff233afadcf84f8846ad13ff038b04f3773d627e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvqX6UmEWBB",
        "outputId": "e954b205-daf1-4b56-92ab-2e8d20d52fd5"
      },
      "source": [
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import contractions\n",
        "from collections import Counter\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from QReLU_TensorFlow_Keras import QReLU\n",
        "from m_QReLU_TensorFlow_Keras import m_QReLU"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxmSYADipo_z"
      },
      "source": [
        "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaXwEcweNYXL"
      },
      "source": [
        "cols = ['sentiment','id','date','query_string','user','text']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwneKfYgO30N"
      },
      "source": [
        "df = pd.read_csv(\"./drive/MyDrive/QReLU/training.csv\",header=None, names=cols,  engine='python')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EXI6Rljkcdo"
      },
      "source": [
        "sentiments_cat = ['negative','neutral','postive']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv3KHgaAkh2G"
      },
      "source": [
        "def chop_dataframe(df, amount):\n",
        "  df_4 = df.loc[df['sentiment'] == 4]\n",
        "  df_1 = df.loc[df['sentiment'] == 0]\n",
        "  df_final = df_4.head(amount)\n",
        "  return pd.concat([df_final, df_1.head(amount)])\n",
        "\n",
        "def encode_corpus(corpus):\n",
        "  words = ' '.join(corpus).split()\n",
        "  \n",
        "  count_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "\n",
        "def count_words(corpus):\n",
        "  words = ' '.join(corpus).split()\n",
        "  \n",
        "  counted_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "\n",
        "  return total_words, counted_words\n",
        "\n",
        "def sort_words(corpus):\n",
        "  total_words, counted_words = count_words(corpus)\n",
        "  return counted_words.most_common(total_words)\n",
        "\n",
        "def decode_sentiments(encoded_sentiments, sentiments_cat=sentiments_cat, postive_index=2, \n",
        "                      negative_index=0, neutral_index=1):\n",
        "  decoded_sentiments = []\n",
        "\n",
        "  for index in range(0, len(encoded_sentiments)):\n",
        "    if encoded_sentiments[index] == 0:\n",
        "      decoded_sentiments.append(sentiments_cat[negative_index])\n",
        "    elif encoded_sentiments[index] == 2:\n",
        "      decoded_sentiments.append(sentiments_cat[neutral_index])\n",
        "    elif encoded_sentiments[index] == 4 or encoded_sentiments[index] == 1:\n",
        "      decoded_sentiments.append(sentiments_cat[postive_index])\n",
        "  \n",
        "  return decoded_sentiments\n",
        "\n",
        "def convert_sentiments(sentiments):\n",
        "  converted = []\n",
        "\n",
        "  for s in sentiments:\n",
        "    if s == 4:\n",
        "      converted.append(1)\n",
        "    else:\n",
        "      converted.append(0)\n",
        "  return np.array(converted)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqZsKAs98qnr"
      },
      "source": [
        "#save load json files\n",
        "import json \n",
        "\n",
        "def save(file_name, obj):\n",
        "  with open(file_name, 'w') as outfile:\n",
        "    json.dump(obj, outfile)\n",
        "\n",
        "def load(file_name):\n",
        "  with open(file_name) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    return data['corpus']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEGaL4GzyyqG"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "def convert_to_lower(corpus):\n",
        "    return [c.lower() for c in corpus]\n",
        "    \n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "def expand_contractions(text):\n",
        "  expanded_words = []    \n",
        "  for word in text.split():\n",
        "    # using contractions.fix to expand the shotened words\n",
        "    expanded_words.append(contractions.fix(word))   \n",
        "      \n",
        "  return' '.join(expanded_words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukTSqtPxXbgI"
      },
      "source": [
        "df_4 = df.loc[df['sentiment'] == 4]\n",
        "df_1 = df.loc[df['sentiment'] == 0]\n",
        "df_final = df_4.head(25000)\n",
        "df_5 = pd.concat([df_final, df_1.head(25000)])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRcMTPmfAuNs"
      },
      "source": [
        "df = df_5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWPSvAyZ79Jn",
        "outputId": "6f018296-ae10-41cf-da9c-8c3929295558"
      },
      "source": [
        "len(df_5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLka4PUkQkP"
      },
      "source": [
        "sentiments = convert_sentiments(df['sentiment'].values)\n",
        "corpus = convert_to_lower(df['text'].values)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MT0Qp9fR7gag",
        "outputId": "cf50b067-721e-4ae2-cad9-94d6597cba21"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"@switchfoot http://twitpic.com/2y1zl - awww, that's a bummer.  you shoulda got david carr of third day to do it. ;d\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObVjkO8GmTQX"
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        \n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        \n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        \n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        \n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        \n",
        "        # lemmatize text\n",
        "        # if text_lemmatization:\n",
        "        #     doc = lemmatize_text(doc)\n",
        "        \n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "            \n",
        "            pat1 = r'@[A-Za-z0-9_]+'\n",
        "            pat2 = r'https?://[^ ]+'\n",
        "            www_pat = r'www.[^ ]+'\n",
        "            combined_pat = r'|'.join((pat1, pat2))\n",
        "            doc = re.sub(combined_pat, '', doc)\n",
        "            doc = re.sub(www_pat, '', doc)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chAYibIDq_tp"
      },
      "source": [
        "normalized_corpus = normalize_corpus(corpus)\n",
        "# save(\"./drive/MyDrive/QReLU/corpus.json\", {\"corpus\": normalized_corpus})\n",
        "\n",
        "# normalized_corpus = load(\"./drive/MyDrive/QReLU/corpus.json\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aEHjzwF9g3o"
      },
      "source": [
        "sentences = normalized_corpus"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dD3157J1TbC"
      },
      "source": [
        "total_words = count_words(normalized_corpus)\n",
        "sorted_words = sort_words(normalized_corpus)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zrlGH-W1tEa"
      },
      "source": [
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emnJ-ViZ1t6R"
      },
      "source": [
        "def tokenize(corpus):\n",
        "  reviews_int = []\n",
        "  for review in corpus:\n",
        "      r = [vocab_to_int[w] for w in review.split()]\n",
        "      reviews_int.append(r)\n",
        "  return reviews_int"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vYm_OFr13Pv",
        "outputId": "4d590fbc-6951-4205-d91e-2d18f976d378"
      },
      "source": [
        "encoded = tokenize(normalized_corpus)\n",
        "encoded_np = np.array(encoded)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9_4673Bz9n",
        "outputId": "d0e6ce5a-4d07-47e8-fa5e-80aa93bdd99c"
      },
      "source": [
        "len(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChL_qqtI4Nzf"
      },
      "source": [
        "#Pre Training Glove embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suleHw375j9p",
        "outputId": "31fee342-85d9-4572-a379-bd7cf31da3b8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = dict()\n",
        "f = open(\"/content/drive/MyDrive/QReLU/glove.6B.100d.txt\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print(\"Loaded %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol-P9qcY5kam"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(normalized_corpus)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8uMr0qw5kwX"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_document = tokenizer.texts_to_sequences(normalized_corpus)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM2eo0YGBLTZ",
        "outputId": "4d28c4a0-d493-403b-b723-b742ead9eaa6"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54261"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4B9b1Nk-sgM"
      },
      "source": [
        "MAXLEN = 160\n",
        "padded_docs = sequence.pad_sequences(encoded_document, MAXLEN, padding='post')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCJxDo3T_vmT"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhX2VT7xANzm"
      },
      "source": [
        "# VOCAB_SIZE = total_words[0]\n",
        "MAXLEN = 160 #the max length of a tweet\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    encoded, sentiments, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, MAXLEN)\n",
        "X_test = sequence.pad_sequences(X_test, MAXLEN)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnWn3s9-AZ3W",
        "outputId": "a333f9ba-9f66-41ff-dde5-e920a0d795f9"
      },
      "source": [
        "len(embedding_matrix), embedding_matrix.size, MAXLEN, vocab_size"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54261, 5426100, 160, 54261)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENKUKKCr6wG5",
        "outputId": "42fb7fda-55d0-4c61-ec64-e7df8ea23f52"
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAXLEN),\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    QReLU()\n",
        "])\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Entity <bound method QReLU.call of <QReLU_TensorFlow_Keras.QReLU object at 0x7f3f5e23b090>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method QReLU.call of <QReLU_TensorFlow_Keras.QReLU object at 0x7f3f5e23b090>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 160, 100)          5426100   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 160, 32)           9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 80, 32)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               49664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "q_re_lu_1 (QReLU)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 5,485,525\n",
            "Trainable params: 5,485,525\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHz5rBIs9ZUJ",
        "outputId": "20af2e28-856d-454a-d22d-389a9b496ba5"
      },
      "source": [
        "model2.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['acc'])\n",
        "\n",
        "history = model2.fit(X_train, y_train, epochs=6, validation_split=0.2, batch_size=32, shuffle=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/6\n",
            "32000/32000 [==============================] - 219s 7ms/sample - loss: 0.7561 - acc: 0.5268 - val_loss: 0.6508 - val_acc: 0.6346\n",
            "Epoch 2/6\n",
            "32000/32000 [==============================] - 215s 7ms/sample - loss: 0.6177 - acc: 0.6881 - val_loss: 0.6106 - val_acc: 0.6570\n",
            "Epoch 3/6\n",
            "32000/32000 [==============================] - 216s 7ms/sample - loss: 0.5548 - acc: 0.7523 - val_loss: 0.5692 - val_acc: 0.7107\n",
            "Epoch 4/6\n",
            "32000/32000 [==============================] - 216s 7ms/sample - loss: 0.4489 - acc: 0.8159 - val_loss: 0.7942 - val_acc: 0.7322\n",
            "Epoch 5/6\n",
            "32000/32000 [==============================] - 218s 7ms/sample - loss: 0.3941 - acc: 0.8543 - val_loss: 0.8156 - val_acc: 0.7191\n",
            "Epoch 6/6\n",
            "32000/32000 [==============================] - 216s 7ms/sample - loss: 0.2646 - acc: 0.9205 - val_loss: 1.0998 - val_acc: 0.7322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vht30w8r9cpu",
        "outputId": "9cca5ea9-4fbd-478c-f00e-41fd9135ac91"
      },
      "source": [
        "results = model2.evaluate(X_test, y_test)\n",
        "print(results)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1267 - acc: 0.7238\n",
            "[1.1267444622039795, 0.7238]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVyVRioYFaqH",
        "outputId": "81793b8e-8ff9-46e2-fc5a-0baac6fbb030"
      },
      "source": [
        "X_test[0], y_test[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,    16,  1812,    96, 44286], dtype=int32), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBm8KcJX-5b5",
        "outputId": "d832e2d0-29d5-4acf-eeac-43eabc0da2b7"
      },
      "source": [
        "model2.predict(X_test[0:2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.42516795],\n",
              "       [0.546085  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G45cUYb7_QzI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}