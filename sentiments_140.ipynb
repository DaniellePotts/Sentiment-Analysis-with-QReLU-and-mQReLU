{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiments 140.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRfY9BTSMeXnt3UEqOzXis",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaniellePotts/Sentiment-Analysis-with-QReLU-and-mQReLU/blob/main/sentiments_140.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d_H-V7CqBWB",
        "outputId": "4be45a22-d890-4c32-d1cf-88aa5020f8ec"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 63.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85440 sha256=e4cdb5c025b178bbb8e156a9fab9948d274641c5c9edf7af09a5517b517b3e4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPL134SmpyhA",
        "outputId": "777a2238-af6b-4c37-c319-06f0f8c0c4ad"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvqX6UmEWBB",
        "outputId": "ca3f6fd5-17f0-4ea2-c7b6-b2c7e14c0fa7"
      },
      "source": [
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import contractions\n",
        "from collections import Counter\n",
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxmSYADipo_z"
      },
      "source": [
        "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaXwEcweNYXL"
      },
      "source": [
        "cols = ['sentiment','id','date','query_string','user','text']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwneKfYgO30N"
      },
      "source": [
        "df = pd.read_csv(\"./training.csv\",header=None, names=cols,  engine='python')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YYpU3triF1H",
        "outputId": "a2037173-b1e6-41c0-966e-0fa70a504cce"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG5QsjOIiuGU",
        "outputId": "3a82ebac-9147-4ca8-b89e-56d2f2eb85b6"
      },
      "source": [
        "df['sentiment'].unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "F5UlEDRjYVe5",
        "outputId": "527fb40a-f40e-4b50-9057-a524d6290e8f"
      },
      "source": [
        "df['sentiment'].value_counts().plot.bar()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbca8155210>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EXI6Rljkcdo"
      },
      "source": [
        "sentiments_cat = ['negative','neutral','postive']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv3KHgaAkh2G"
      },
      "source": [
        "def encode_corpus(corpus):\n",
        "  words = ' '.join(corpus).split()\n",
        "  \n",
        "  count_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "\n",
        "def count_words(corpus):\n",
        "  words = ' '.join(corpus).split()\n",
        "  \n",
        "  counted_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "\n",
        "  return total_words, counted_words\n",
        "\n",
        "def sort_words(corpus):\n",
        "  total_words, counted_words = count_words(corpus)\n",
        "  return counted_words.most_common(total_words)\n",
        "\n",
        "def decode_sentiments(encoded_sentiments, sentiments_cat=sentiments_cat, postive_index=2, \n",
        "                      negative_index=0, neutral_index=1):\n",
        "  decoded_sentiments = []\n",
        "\n",
        "  for index in range(0, len(encoded_sentiments)):\n",
        "    if encoded_sentiments[index] == 0:\n",
        "      decoded_sentiments.append(sentiments_cat[negative_index])\n",
        "    elif encoded_sentiments[index] == 2:\n",
        "      decoded_sentiments.append(sentiments_cat[neutral_index])\n",
        "    elif encoded_sentiments[index] == 4 or encoded_sentiments[index] == 1:\n",
        "      decoded_sentiments.append(sentiments_cat[postive_index])\n",
        "  \n",
        "  return decoded_sentiments\n",
        "\n",
        "def convert_sentiments(sentiments):\n",
        "  converted = []\n",
        "\n",
        "  for s in sentiments:\n",
        "    if s == 4:\n",
        "      converted.append(1)\n",
        "    else:\n",
        "      converted.append(0)\n",
        "  return np.array(converted)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEGaL4GzyyqG"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "def convert_to_lower(corpus):\n",
        "    return [c.lower() for c in corpus]\n",
        "    \n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "def expand_contractions(text):\n",
        "  expanded_words = []    \n",
        "  for word in text.split():\n",
        "    # using contractions.fix to expand the shotened words\n",
        "    expanded_words.append(contractions.fix(word))   \n",
        "      \n",
        "  return' '.join(expanded_words)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhBfhpNoBpEi"
      },
      "source": [
        "sentiments = convert_sentiments(df['sentiment'].values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLka4PUkQkP"
      },
      "source": [
        "sentiments = convert_sentiments(df['sentiment'].values)\n",
        "corpus = convert_to_lower(df['text'].values)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObVjkO8GmTQX"
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        \n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        \n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        \n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        \n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        \n",
        "        # lemmatize text\n",
        "        # if text_lemmatization:\n",
        "        #     doc = lemmatize_text(doc)\n",
        "        \n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "            \n",
        "            pat1 = r'@[A-Za-z0-9_]+'\n",
        "            pat2 = r'https?://[^ ]+'\n",
        "            www_pat = r'www.[^ ]+'\n",
        "            combined_pat = r'|'.join((pat1, pat2))\n",
        "            doc = re.sub(combined_pat, '', doc)\n",
        "            doc = re.sub(www_pat, '', doc)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chAYibIDq_tp"
      },
      "source": [
        "normalized_corpus = normalize_corpus(corpus)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dD3157J1TbC"
      },
      "source": [
        "total_words = count_words(normalized_corpus)\n",
        "sorted_words = sort_words(normalized_corpus)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zrlGH-W1tEa"
      },
      "source": [
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFA7H0o517FZ"
      },
      "source": [
        "vocab_to_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emnJ-ViZ1t6R"
      },
      "source": [
        "def tokenize(corpus):\n",
        "  reviews_int = []\n",
        "  for review in corpus:\n",
        "      r = [vocab_to_int[w] for w in review.split()]\n",
        "      reviews_int.append(r)\n",
        "  return reviews_int"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vYm_OFr13Pv",
        "outputId": "109dda2b-c710-48ae-d847-9d8c0e1be129"
      },
      "source": [
        "encoded = tokenize(normalized_corpus)\n",
        "encoded_np = np.array(encoded)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITilXuY2sU0i"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcVqyv1-ngt7"
      },
      "source": [
        "VOCAB_SIZE = 12167406\n",
        "MAXLEN = 100\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    encoded, sentiments, test_size=0.33, random_state=42)\n",
        "#we cannot pass different length data to the model, thus all must be the same so we normalise them with MAXLEN\n",
        "#if the review is > maxlen, trim it, if not pad it with 0s \n",
        "X_train = sequence.pad_sequences(X_train, MAXLEN)\n",
        "X_test = sequence.pad_sequences(X_test, MAXLEN)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iji0jADdxtQk",
        "outputId": "f1ade229-58bf-4e0e-cf51-85b2ff67615b"
      },
      "source": [
        "len(X_train), len(X_test)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1072000, 528000)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpLn6oGQotXT",
        "outputId": "5b7bca01-e962-41a9-c9af-6b106f640e55"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0, 96113,   529,     1,     2,   836,\n",
              "           9], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwxdSKj96E8C"
      },
      "source": [
        "\"\"\"Additional utility for Deep Learning models in TensorFlow and Keras\"\"\"\n",
        "\n",
        "# The Quantum ReLU or 'QReLU' as a custom activation function in TensorFlow (tf_q_relu)\n",
        "# and Keras (QReLU)\n",
        "\n",
        "# Author: Luca Parisi <luca.parisi@ieee.org>\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "\n",
        "# QReLU as a custom activation function in TensorFlow\n",
        "\n",
        "'''\n",
        "# Example of usage of the QReLU in TensorFlow as a custom activation function of a convolutional layer (#2)\n",
        "\n",
        "convolutional_layer_2 = tf.layers.conv2d(\n",
        "                        inputs=pooling_layer_1,\n",
        "                        filters=64,\n",
        "                        kernel_size=[5, 5],\n",
        "                        padding=\"same\")\n",
        "convolutional_layer_activation = tf_q_relu(convolutional_layer_2)\n",
        "pooling_layer_2 = tf.layers.max_pooling2d(inputs=convolutional_layer_activation, pool_size=[2, 2], strides=2)\n",
        "'''\n",
        "\n",
        "# Defining the QReLU function\n",
        "def q_relu(x):\n",
        "  if x>0:\n",
        "    x = x\n",
        "    return x\n",
        "  else:\n",
        "    x = 0.01*x-2*x\n",
        "    return x\n",
        "\n",
        "# Vectorising the QReLU function  \n",
        "np_q_relu = np.vectorize(q_relu)\n",
        "\n",
        "# Defining the derivative of the function QReLU\n",
        "def d_q_relu(x):\n",
        "  if x>0:\n",
        "    x = 1\n",
        "    return x\n",
        "  else:\n",
        "    x = 0.01-2\n",
        "    return x\n",
        "\n",
        "# Vectorising the derivative of the QReLU function  \n",
        "np_d_q_relu = np.vectorize(d_q_relu)\n",
        "\n",
        "# Defining the gradient function of the QReLU\n",
        "def q_relu_grad(op, grad):\n",
        "    x = op.inputs[0]\n",
        "    n_gr = tf_d_q_relu(x)\n",
        "    return grad * n_gr\n",
        "\n",
        "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
        "# Generating a unique name to avoid duplicates:\n",
        "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+2))\n",
        "    tf.RegisterGradient(rnd_name)(grad)\n",
        "    g = tf.get_default_graph()\n",
        "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
        "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
        "\n",
        "np_q_relu_32 = lambda x: np_q_relu(x).astype(np.float32)\n",
        "\n",
        "def tf_q_relu(x,name=None):\n",
        "    with tf.name_scope(name, \"q_relu\", [x]) as name:\n",
        "        y = py_func(np_q_relu_32,   # Forward pass function\n",
        "                        [x],\n",
        "                        [tf.float32],\n",
        "                        name=name,\n",
        "                         grad= q_relu_grad) # The function that overrides gradient\n",
        "        y[0].set_shape(x.get_shape())     # To specify the rank of the input.\n",
        "        return y[0]\n",
        "\n",
        "np_d_q_relu_32 = lambda x: np_d_q_relu(x).astype(np.float32)\n",
        "\n",
        "def tf_d_q_relu(x,name=None):\n",
        "    with tf.name_scope(name, \"d_q_relu\", [x]) as name:\n",
        "        y = tf.py_func(np_d_q_relu_32,\n",
        "                        [x],\n",
        "                        [tf.float32],\n",
        "                        name=name,\n",
        "                        stateful=False)\n",
        "        return y[0]\n",
        "\n",
        "\n",
        "# QReLU as a custom layer in Keras \n",
        "\n",
        "'''\n",
        "# Example of usage of the QReLU as a Keras layer in a sequential model between a convolutional layer and a pooling layer\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(QReLU())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "'''\n",
        "\n",
        "class QReLU(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(QReLU,self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs,name=None):\n",
        "        return tf_q_relu(inputs,name=None)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(QReLU, self).get_config()\n",
        "        return dict(list(base_config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENKUKKCr6wG5",
        "outputId": "fc272e01-029f-46f4-ad9b-01d7e81287b4"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    QReLU()\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Entity <bound method QReLU.call of <__main__.QReLU object at 0x7fbca111dd10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method QReLU.call of <__main__.QReLU object at 0x7fbca111dd10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-57-973dbd44c679>:64: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          389356992 \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               295936    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "q_re_lu (QReLU)              (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 389,653,185\n",
            "Trainable params: 389,653,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHz5rBIs9ZUJ",
        "outputId": "01267a0e-89ca-4403-f4d7-84c12c610835"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 857600 samples, validate on 214400 samples\n",
            "Epoch 1/5\n",
            "857600/857600 [==============================] - 4880s 6ms/sample - loss: 7.5831 - acc: 0.5012 - val_loss: 7.6386 - val_acc: 0.4991\n",
            "Epoch 2/5\n",
            "857600/857600 [==============================] - 4804s 6ms/sample - loss: 7.6752 - acc: 0.4994 - val_loss: 7.6386 - val_acc: 0.4991\n",
            "Epoch 3/5\n",
            "857600/857600 [==============================] - 4804s 6ms/sample - loss: 7.6752 - acc: 0.4994 - val_loss: 7.6386 - val_acc: 0.4991\n",
            "Epoch 4/5\n",
            " 85568/857600 [=>............................] - ETA: 1:09:46 - loss: 7.6786 - acc: 0.4992"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vht30w8r9cpu"
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z63se7tbrmJ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}